# -*- coding: utf-8 -*-
"""Task2_Assignement2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/109neOLMPQZOmq_JTRjJGRc9BRT2eS0pQ

**a.) PCA from scratch and then GMM is performed**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

train_data = pd.read_csv('/content/drive/MyDrive/mnist_train.csv')
test_data = pd.read_csv('/content/drive/MyDrive/mnist_test.csv')

X_train = train_data.iloc[:, 1:].values
y_train = train_data.iloc[:, 0].values
X_test = test_data.iloc[:, 1:].values
y_test = test_data.iloc[:, 0].values

X_train.shape

from sklearn.mixture import GaussianMixture


X_train_mean = np.mean(X_train, axis=0)
X_train_centered = X_train - X_train_mean

PCA_components = [32, 64, 128]
clusters_list = [10, 7, 4]
reduced_data = []

def pca(X, n_components):
  U, S, Vt = np.linalg.svd(X, full_matrices=False)
  reduced_training_data = np.dot(U[:, :n_components], np.diag(S[:n_components]))
  return reduced_training_data

for n_components in PCA_components:
  reduced_training_data = pca(X_train_centered, n_components)
  reduced_data.append(reduced_training_data)


  for n_clusters in clusters_list:
    gmm = GaussianMixture(n_components=n_clusters, random_state = 0)
    cluster_labels = gmm.fit_predict(reduced_training_data)

"""**b.) Visualization of 5 random samples from each clusters of 10, 7 and 4 clusters corresponding to 32, 64 and 128 PCA components**"""

def visualize_clusters(cluster_labels, n_clusters, X, num_samples=5, pca_components=None):
    fig, axes = plt.subplots(n_clusters, num_samples, figsize=(10, 8))

    for cluster in range(n_clusters):
        cluster_indices = np.where(cluster_labels == cluster)[0]
        random_sample_indices = np.random.choice(cluster_indices, size=num_samples, replace=False)
        plt.suptitle(f'PCA={n_components}, Clusters={n_clusters}')

        for i, sample_idx in enumerate(random_sample_indices):
            image = X[sample_idx].reshape(28, 28)
            axes[cluster, i].imshow(image, cmap='gray')
            axes[cluster, i].axis('off')
            if i == 0:
                axes[cluster, i].set_title(f'Cluster {cluster}')

    plt.tight_layout()
    plt.show()

for i, n_components in enumerate(PCA_components):
    reduced_training_data = reduced_data[i]
    for n_clusters in clusters_list:
      gmm = GaussianMixture(n_components=n_clusters, random_state=0)
      cluster_labels = gmm.fit_predict(reduced_training_data)
      visualize_clusters(cluster_labels, n_clusters, X_train, pca_components=n_components)

"""**d.) Optimal number of PCA components**

**Performed using PCA library so that the explained_variance_ratio attribute of PCA cab be used directly**

From elbow point, we may guess the optimal value of PCA component which is around 200
"""

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

X_train_std = StandardScaler().fit_transform(X_train)

pca = PCA()
pca.fit(X_train_std)
cumulative_variance = pca.explained_variance_ratio_.cumsum()
plt.figure(figsize=(8, 6))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Cumulative Explained Variance Plot')
plt.grid()
plt.show()